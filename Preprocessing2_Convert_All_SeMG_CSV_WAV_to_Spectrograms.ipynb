{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dISB8U__ym2I"
   },
   "source": [
    "# Convert all csv files to wav files and add their corresponding labels as csv files in output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "058sy1rcyViQ",
    "outputId": "db0b65de-b757-4f7b-b0fb-31a27f47d72b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio files and labels have been successfully created and stored in the 'FixingAudioSpectrograms' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def normalize_and_convert_to_wav(input_directory, output_directory, sample_rate=2000):\n",
    "    \"\"\"\n",
    "    Normalizes CSV data and converts it to WAV files, while saving labels to separate CSV files.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing the input CSV files.\n",
    "        output_directory (str): Path to the directory where the output WAV and label CSV files will be saved.\n",
    "        sample_rate (int, optional): The sample rate for the WAV files. Default is 2000 Hz.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Get a list of all CSV files in the input directory\n",
    "    csv_files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Normalize and convert to WAV for each file individually\n",
    "    for csv_file in csv_files:\n",
    "        data = pd.read_csv(os.path.join(input_directory, csv_file))\n",
    "        first_column = data.iloc[:, 0]\n",
    "\n",
    "        # Normalize the data based on the file's own maximum value\n",
    "        normalized_data = np.int16((first_column / first_column.max()) * 32767)\n",
    "\n",
    "        # Write the values to an audio file with the specified sample rate\n",
    "        output_file = os.path.join(output_directory, os.path.splitext(csv_file)[0] + '.wav')\n",
    "        write(output_file, sample_rate, normalized_data)\n",
    "\n",
    "        # Save the labels to a new CSV file\n",
    "        label_output_file = os.path.join(output_directory, os.path.splitext(csv_file)[0] + '_labels.csv')\n",
    "        labels = data.drop(data.columns[0], axis=1)\n",
    "        labels.to_csv(label_output_file, index=False)\n",
    "\n",
    "    print(\"Audio files and labels have been successfully created and stored in the ./Orig_WAV-Label_Files directory.\")\n",
    "\n",
    "input_dir =\"./Orig_CSV_Files\"\n",
    "output_dir = \"./Orig_WAV-Label_Files\"\n",
    "normalize_and_convert_to_wav(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d6UJfIjzEGT",
    "outputId": "2d9eed19-73af-43e7-bac1-a0173046bdf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAV Files: ['./Orig_WAV-Label_Files/07_swallow_banana_N2.wav', './Orig_WAV-Label_Files/23_swallow_water_N2.wav', './Orig_WAV-Label_Files/06_swallow_water.wav', './Orig_WAV-Label_Files/20_swallow_dry.wav', './Orig_WAV-Label_Files/23_swallow_banana.wav', './Orig_WAV-Label_Files/04_swallow_dry.wav', './Orig_WAV-Label_Files/03_swallow_water.wav', './Orig_WAV-Label_Files/19_swallow_banana_N2.wav', './Orig_WAV-Label_Files/13_swallow_banana.wav', './Orig_WAV-Label_Files/14_swallow_water_N2.wav', './Orig_WAV-Label_Files/03_swallow_banana.wav', './Orig_WAV-Label_Files/12_swallow_dry.wav', './Orig_WAV-Label_Files/15_swallow_dry.wav', './Orig_WAV-Label_Files/07_swallow_water.wav', './Orig_WAV-Label_Files/05_swallow_banana_N2.wav', './Orig_WAV-Label_Files/10_swallow_banana.wav', './Orig_WAV-Label_Files/02_swallow_water.wav', './Orig_WAV-Label_Files/20_swallow_banana.wav', './Orig_WAV-Label_Files/03_swallow_dry.wav', './Orig_WAV-Label_Files/27_swallow_dry.wav', './Orig_WAV-Label_Files/04_swallow_water.wav', './Orig_WAV-Label_Files/14_swallow_dry.wav', './Orig_WAV-Label_Files/13b_swallow_banana.wav', './Orig_WAV-Label_Files/26_swallow_banana.wav', './Orig_WAV-Label_Files/27_swallow_banana_N2.wav', './Orig_WAV-Label_Files/18_swallow_water.wav', './Orig_WAV-Label_Files/28_swallow_dry_held_larynx_at_top.wav', './Orig_WAV-Label_Files/01_swallow_water.wav', './Orig_WAV-Label_Files/06_swallow_banana.wav', './Orig_WAV-Label_Files/02_swallow_dry.wav', './Orig_WAV-Label_Files/15_swallow_banana_N2.wav', './Orig_WAV-Label_Files/16_swallow_banana.wav', './Orig_WAV-Label_Files/27_swallow_water_N2.wav', './Orig_WAV-Label_Files/26_swallow_dry.wav', './Orig_WAV-Label_Files/03_swallow_banana_N3.wav', './Orig_WAV-Label_Files/15_swallow_banana.wav', './Orig_WAV-Label_Files/25_swallow_banana_N2.wav', './Orig_WAV-Label_Files/21_swallow_dry.wav', './Orig_WAV-Label_Files/18_swallow_banana.wav', './Orig_WAV-Label_Files/05_swallow_water.wav', './Orig_WAV-Label_Files/08_swallow_banana.wav', './Orig_WAV-Label_Files/05_swallow_banana.wav', './Orig_WAV-Label_Files/19_swallow_water.wav', './Orig_WAV-Label_Files/05_swallow_dry.wav', './Orig_WAV-Label_Files/09_swallow_banana_N2.wav', './Orig_WAV-Label_Files/01_swallow_banana_N2.wav', './Orig_WAV-Label_Files/13_swallow_dry.wav', './Orig_WAV-Label_Files/25_swallow_banana.wav', './Orig_WAV-Label_Files/17_swallow_banana_N2.wav', './Orig_WAV-Label_Files/09_swallow_banana.wav', './Orig_WAV-Label_Files/28_swallow_water_continuously.wav', './Orig_WAV-Label_Files/04_swallow_banana.wav', './Orig_WAV-Label_Files/21_swallow_water.wav', './Orig_WAV-Label_Files/09_swallow_water.wav', './Orig_WAV-Label_Files/18_swallow_banana_N2.wav', './Orig_WAV-Label_Files/14_swallow_banana.wav', './Orig_WAV-Label_Files/19_swallow_banana.wav', './Orig_WAV-Label_Files/15_swallow_water.wav', './Orig_WAV-Label_Files/16_swallow_dry.wav', './Orig_WAV-Label_Files/22_swallow_banana_N2.wav', './Orig_WAV-Label_Files/04_swallow_water_N2.wav', './Orig_WAV-Label_Files/24_swallow_water.wav', './Orig_WAV-Label_Files/10_swallow_banana_N2.wav', './Orig_WAV-Label_Files/24_swallow_banana.wav', './Orig_WAV-Label_Files/19_swallow_dry.wav', './Orig_WAV-Label_Files/24_swallow_dry.wav', './Orig_WAV-Label_Files/10_swallow_water.wav', './Orig_WAV-Label_Files/10_swallow_banana_N3.wav', './Orig_WAV-Label_Files/20_swallow_water.wav', './Orig_WAV-Label_Files/08_swallow_water.wav', './Orig_WAV-Label_Files/07_swallow_dry.wav', './Orig_WAV-Label_Files/23_swallow_dry.wav', './Orig_WAV-Label_Files/14_swallow_water.wav', './Orig_WAV-Label_Files/27_swallow_banana.wav', './Orig_WAV-Label_Files/25_swallow_water.wav', './Orig_WAV-Label_Files/11_swallow_dry.wav', './Orig_WAV-Label_Files/17_swallow_banana.wav', './Orig_WAV-Label_Files/16_swallow_water_N2.wav', './Orig_WAV-Label_Files/07_swallow_banana.wav', './Orig_WAV-Label_Files/11_swallow_water.wav', './Orig_WAV-Label_Files/08_swallow_dry.wav', './Orig_WAV-Label_Files/14_swallow_banana_N2.wav', './Orig_WAV-Label_Files/23_swallow_water.wav', './Orig_WAV-Label_Files/06_swallow_dry.wav', './Orig_WAV-Label_Files/02_swallow_banana_N2.wav', './Orig_WAV-Label_Files/11_swallow_banana.wav', './Orig_WAV-Label_Files/29_swallow_water_continuously.wav', './Orig_WAV-Label_Files/17_swallow_water.wav', './Orig_WAV-Label_Files/22_swallow_dry.wav', './Orig_WAV-Label_Files/01_swallow_banana.wav', './Orig_WAV-Label_Files/26_swallow_water.wav', './Orig_WAV-Label_Files/10_swallow_dry.wav', './Orig_WAV-Label_Files/12_swallow_water.wav', './Orig_WAV-Label_Files/09_swallow_dry.wav', './Orig_WAV-Label_Files/28_swallow_continuous_water.wav', './Orig_WAV-Label_Files/21_swallow_banana.wav', './Orig_WAV-Label_Files/22_swallow_banana.wav', './Orig_WAV-Label_Files/22_swallow_water.wav', './Orig_WAV-Label_Files/17_swallow_water_N2.wav', './Orig_WAV-Label_Files/17_swallow_dry.wav', './Orig_WAV-Label_Files/16_swallow_water.wav', './Orig_WAV-Label_Files/16_swallow_banana_N2.wav', './Orig_WAV-Label_Files/27_swallow_water.wav', './Orig_WAV-Label_Files/18_swallow_dry.wav', './Orig_WAV-Label_Files/25_swallow_dry.wav', './Orig_WAV-Label_Files/02_swallow_banana.wav', './Orig_WAV-Label_Files/13_swallow_water.wav', './Orig_WAV-Label_Files/24_swallow_banana_N3.wav', './Orig_WAV-Label_Files/12_swallow_banana.wav', './Orig_WAV-Label_Files/01_swallow_dry.wav']\n",
      "CSV Files: ['./Orig_CSV_Files/09_swallow_banana.csv', './Orig_CSV_Files/21_swallow_water.csv', './Orig_CSV_Files/04_swallow_banana.csv', './Orig_CSV_Files/28_swallow_water_continuously.csv', './Orig_CSV_Files/09_swallow_water.csv', './Orig_CSV_Files/18_swallow_banana_N2.csv', './Orig_CSV_Files/14_swallow_banana.csv', './Orig_CSV_Files/15_swallow_water.csv', './Orig_CSV_Files/19_swallow_banana.csv', './Orig_CSV_Files/16_swallow_dry.csv', './Orig_CSV_Files/04_swallow_water_N2.csv', './Orig_CSV_Files/22_swallow_banana_N2.csv', './Orig_CSV_Files/10_swallow_banana_N2.csv', './Orig_CSV_Files/24_swallow_water.csv', './Orig_CSV_Files/24_swallow_banana.csv', './Orig_CSV_Files/24_swallow_dry.csv', './Orig_CSV_Files/19_swallow_dry.csv', './Orig_CSV_Files/10_swallow_water.csv', './Orig_CSV_Files/10_swallow_banana_N3.csv', './Orig_CSV_Files/20_swallow_water.csv', './Orig_CSV_Files/08_swallow_water.csv', './Orig_CSV_Files/07_swallow_dry.csv', './Orig_CSV_Files/23_swallow_dry.csv', './Orig_CSV_Files/14_swallow_water.csv', './Orig_CSV_Files/27_swallow_banana.csv', './Orig_CSV_Files/25_swallow_water.csv', './Orig_CSV_Files/17_swallow_banana.csv', './Orig_CSV_Files/11_swallow_dry.csv', './Orig_CSV_Files/16_swallow_water_N2.csv', './Orig_CSV_Files/11_swallow_water.csv', './Orig_CSV_Files/07_swallow_banana.csv', './Orig_CSV_Files/08_swallow_dry.csv', './Orig_CSV_Files/06_swallow_dry.csv', './Orig_CSV_Files/23_swallow_water.csv', './Orig_CSV_Files/14_swallow_banana_N2.csv', './Orig_CSV_Files/02_swallow_banana_N2.csv', './Orig_CSV_Files/11_swallow_banana.csv', './Orig_CSV_Files/29_swallow_water_continuously.csv', './Orig_CSV_Files/17_swallow_water.csv', './Orig_CSV_Files/01_swallow_banana.csv', './Orig_CSV_Files/22_swallow_dry.csv', './Orig_CSV_Files/26_swallow_water.csv', './Orig_CSV_Files/10_swallow_dry.csv', './Orig_CSV_Files/09_swallow_dry.csv', './Orig_CSV_Files/12_swallow_water.csv', './Orig_CSV_Files/21_swallow_banana.csv', './Orig_CSV_Files/28_swallow_continuous_water.csv', './Orig_CSV_Files/22_swallow_banana.csv', './Orig_CSV_Files/17_swallow_water_N2.csv', './Orig_CSV_Files/22_swallow_water.csv', './Orig_CSV_Files/17_swallow_dry.csv', './Orig_CSV_Files/16_swallow_banana_N2.csv', './Orig_CSV_Files/16_swallow_water.csv', './Orig_CSV_Files/27_swallow_water.csv', './Orig_CSV_Files/25_swallow_dry.csv', './Orig_CSV_Files/18_swallow_dry.csv', './Orig_CSV_Files/02_swallow_banana.csv', './Orig_CSV_Files/13_swallow_water.csv', './Orig_CSV_Files/24_swallow_banana_N3.csv', './Orig_CSV_Files/01_swallow_dry.csv', './Orig_CSV_Files/12_swallow_banana.csv', './Orig_CSV_Files/06_swallow_water.csv', './Orig_CSV_Files/23_swallow_water_N2.csv', './Orig_CSV_Files/07_swallow_banana_N2.csv', './Orig_CSV_Files/20_swallow_dry.csv', './Orig_CSV_Files/23_swallow_banana.csv', './Orig_CSV_Files/04_swallow_dry.csv', './Orig_CSV_Files/03_swallow_water.csv', './Orig_CSV_Files/19_swallow_banana_N2.csv', './Orig_CSV_Files/13_swallow_banana.csv', './Orig_CSV_Files/14_swallow_water_N2.csv', './Orig_CSV_Files/12_swallow_dry.csv', './Orig_CSV_Files/03_swallow_banana.csv', './Orig_CSV_Files/15_swallow_dry.csv', './Orig_CSV_Files/07_swallow_water.csv', './Orig_CSV_Files/05_swallow_banana_N2.csv', './Orig_CSV_Files/10_swallow_banana.csv', './Orig_CSV_Files/02_swallow_water.csv', './Orig_CSV_Files/20_swallow_banana.csv', './Orig_CSV_Files/03_swallow_dry.csv', './Orig_CSV_Files/27_swallow_dry.csv', './Orig_CSV_Files/04_swallow_water.csv', './Orig_CSV_Files/13b_swallow_banana.csv', './Orig_CSV_Files/14_swallow_dry.csv', './Orig_CSV_Files/26_swallow_banana.csv', './Orig_CSV_Files/27_swallow_banana_N2.csv', './Orig_CSV_Files/28_swallow_dry_held_larynx_at_top.csv', './Orig_CSV_Files/18_swallow_water.csv', './Orig_CSV_Files/02_swallow_dry.csv', './Orig_CSV_Files/06_swallow_banana.csv', './Orig_CSV_Files/01_swallow_water.csv', './Orig_CSV_Files/15_swallow_banana_N2.csv', './Orig_CSV_Files/16_swallow_banana.csv', './Orig_CSV_Files/03_swallow_banana_N3.csv', './Orig_CSV_Files/26_swallow_dry.csv', './Orig_CSV_Files/27_swallow_water_N2.csv', './Orig_CSV_Files/21_swallow_dry.csv', './Orig_CSV_Files/25_swallow_banana_N2.csv', './Orig_CSV_Files/15_swallow_banana.csv', './Orig_CSV_Files/05_swallow_water.csv', './Orig_CSV_Files/18_swallow_banana.csv', './Orig_CSV_Files/08_swallow_banana.csv', './Orig_CSV_Files/05_swallow_banana.csv', './Orig_CSV_Files/19_swallow_water.csv', './Orig_CSV_Files/09_swallow_banana_N2.csv', './Orig_CSV_Files/05_swallow_dry.csv', './Orig_CSV_Files/01_swallow_banana_N2.csv', './Orig_CSV_Files/13_swallow_dry.csv', './Orig_CSV_Files/25_swallow_banana.csv', './Orig_CSV_Files/17_swallow_banana_N2.csv']\n"
     ]
    }
   ],
   "source": [
    "# Verify directory contents\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "wav_path = \"./Orig_WAV-Label_Files\"\n",
    "csv_path = \"./Orig_CSV_Files\"\n",
    "\n",
    "wav_files = glob.glob(os.path.join(wav_path, '*.wav'))\n",
    "print(\"WAV Files:\", wav_files)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(csv_path, '*.csv'))\n",
    "print(\"CSV Files:\", csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "UXQYbmQnyOId",
    "outputId": "9e2e71dd-394e-4204-faf2-7d97137df23a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pyl5Cp2wsGA",
    "outputId": "1d3530a0-ccf0-4868-8ac1-8a0d3676dcd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of segments: 9503\n",
      "Total number of zero labels: 7243\n",
      "Total number of one labels: 1216\n",
      "Total number of files: 110\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_vmap(spectrogram):\n",
    "    \"\"\"\n",
    "    Creates the vmap (velocity map) of a given spectrogram. The vmap represents the temporal derivative\n",
    "    of the spectrogram, providing information about the changes in frequency content over time.\n",
    "\n",
    "    Args:\n",
    "        spectrogram (numpy.ndarray): Input spectrogram with shape (T, f), where T is the number of time frames\n",
    "                                     and f is the number of frequency bins.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The vmap of the spectrogram with the same shape as the input spectrogram.\n",
    "    \"\"\"\n",
    "    T, f = spectrogram.shape\n",
    "\n",
    "    vmap = np.zeros_like(spectrogram)\n",
    "\n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            vmap[t, :] = spectrogram[t + 1, :] - spectrogram[t, :]\n",
    "        elif t == T - 1:\n",
    "            vmap[t, :] = spectrogram[t, :] - spectrogram[t - 1, :]\n",
    "        else:\n",
    "            vmap[t, :] = (spectrogram[t + 1, :] - spectrogram[t - 1, :]) / 2\n",
    "\n",
    "    return vmap\n",
    "\n",
    "def create_amap(vmap):\n",
    "    \"\"\"\n",
    "    Creates the amap (acceleration map) of a given vmap. The amap represents the second temporal derivative\n",
    "    of the spectrogram, providing information about the acceleration of changes in frequency content over time.\n",
    "\n",
    "    Args:\n",
    "        vmap (numpy.ndarray): Input vmap with shape (T, f), where T is the number of time frames\n",
    "                              and f is the number of frequency bins.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The amap of the vmap with the same shape as the input vmap.\n",
    "    \"\"\"\n",
    "    T, f = vmap.shape\n",
    "\n",
    "    amap = np.zeros_like(vmap)\n",
    "\n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            amap[t, :] = vmap[t + 1, :] - vmap[t, :]\n",
    "        elif t == T - 1:\n",
    "            amap[t, :] = vmap[t, :] - vmap[t - 1, :]\n",
    "        else:\n",
    "            amap[t, :] = (vmap[t + 1, :] - vmap[t - 1, :]) / 2\n",
    "\n",
    "    return amap\n",
    "\n",
    "\n",
    "def process_audio_file(audio_file_path, csv_label_path, output_path, segment_length=2, step_size_in_sec=0.1, label_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Processes an audio file by creating segments of mel spectrograms, velocity maps (vmap), and acceleration maps (amap),\n",
    "    and saves them as tensors along with their labels.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path (str): Path to the input audio file.\n",
    "        csv_label_path (str): Path to the CSV file containing labels.\n",
    "        output_path (str): Path to the output directory where the tensors will be saved.\n",
    "        segment_length (int, optional): Length of each segment in seconds. Defaults to 2.\n",
    "        step_size_in_sec (float, optional): Step size in seconds for creating segments. Defaults to 0.1.\n",
    "        label_threshold (float, optional): Threshold for determining the most common label based on the percentage\n",
    "                                           of labels in the segment. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_file_path, sr=None)\n",
    "    \n",
    "    # Append threshold and step size to output directory\n",
    "    output_path = f\"{output_path}_thresh_{label_threshold}_step_{step_size_in_sec}\"\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Compute the mel spectrogram with the specified settings\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=31,\n",
    "                                       fmin=1, fmax=1000, window='hann',\n",
    "                                       n_mels=128, power=2.0, center=False)\n",
    "\n",
    "    # Convert to decibels\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Load the CSV labels\n",
    "    labels = pd.read_csv(csv_label_path).iloc[:, 0].values\n",
    "\n",
    "    # Calculate segment and step sizes in frames\n",
    "    num_segments = int(len(y)/(step_size_in_sec*sr)-(sr*2)/(step_size_in_sec*sr)) #calculate the number of segments in terms of spectrogram frames\n",
    "    step_size_spectrograms = S_dB.shape[1] // num_segments # compute the step size in terms of spectrogram size\n",
    "\n",
    "    # Initialize counters\n",
    "    num_zero_labels = 0\n",
    "    num_one_labels = 0\n",
    "    start_frame_spectrograms = 0\n",
    "    start_frame_labels = 0\n",
    "\n",
    "    # Loop through the audio file and create segments\n",
    "    for i in range(num_segments):\n",
    "        # Calculate end frame of segment\n",
    "        end_frame_spectrograms = start_frame_spectrograms + 128\n",
    "        end_frame_labels = min(int(len(labels)-1), int(end_frame_spectrograms * (len(labels) / len(S_dB[0]))))\n",
    "    \n",
    "        segment = S_dB[:, start_frame_spectrograms:end_frame_spectrograms]\n",
    "\n",
    "        segment_labels = labels[int(start_frame_labels):int(end_frame_labels)]\n",
    "\n",
    "    \n",
    "        # Ensure the segment has the correct size and skip if not\n",
    "        if segment.shape[1] != 128 or len(segment_labels) == 0:\n",
    "            start_frame_spectrograms += step_size_spectrograms\n",
    "            start_frame_labels = int(start_frame_spectrograms * (len(labels) / len(S_dB[0])))\n",
    "            continue\n",
    "    \n",
    "        # Create vmap and amap\n",
    "        vmap = create_vmap(segment)\n",
    "        amap = create_amap(vmap)\n",
    "    \n",
    "        # Determine the label based on the majority rule\n",
    "        if np.sum(segment_labels == 1) / len(segment_labels) > label_threshold:\n",
    "            most_common_label = 1\n",
    "        else:\n",
    "            most_common_label = 0\n",
    "    \n",
    "        # Update counters\n",
    "        if most_common_label == 0:\n",
    "            num_zero_labels += 1\n",
    "        elif most_common_label == 1:\n",
    "            num_one_labels += 1\n",
    "    \n",
    "        # Combine the spectrogram, vmap, and amap into a 3x128x128 tensor\n",
    "        combined_tensor = torch.tensor(np.stack([segment, vmap, amap], axis=0), dtype=torch.float32)\n",
    "    \n",
    "        # Store corresponding label in the label tensor\n",
    "        label_tensor = torch.tensor(most_common_label, dtype=torch.int64)\n",
    "    \n",
    "        # Create segment name\n",
    "        segment_name = f\"{os.path.splitext(os.path.basename(audio_file_path))[0]}_segment_{i:04d}\"\n",
    "    \n",
    "        # Save Spectrogram Tensor\n",
    "        combined_tensor_path = os.path.join(output_path, f\"{segment_name}_combined.pt\")\n",
    "        torch.save(combined_tensor, combined_tensor_path)\n",
    "    \n",
    "        # Save Label Tensor\n",
    "        label_tensor_path = os.path.join(output_path, f\"{segment_name}_label.pt\")\n",
    "        torch.save(label_tensor, label_tensor_path)\n",
    "    \n",
    "        # Update start frames\n",
    "        start_frame_spectrograms += step_size_spectrograms\n",
    "        start_frame_labels = int(start_frame_spectrograms * (len(labels) / len(S_dB[0])))\n",
    "\n",
    "    return num_segments, num_zero_labels, num_one_labels\n",
    "\n",
    "# Example usage\n",
    "input_path = './Orig_WAV-Label_Files'\n",
    "output_path = \"./FINISHED_PRODUCT\"\n",
    "\n",
    "# Iterate through all label and audio files in the directory\n",
    "tot_segments = 0\n",
    "tot_zeros = 0\n",
    "tot_ones = 0\n",
    "tot_files = 0\n",
    "label_threshold = .5\n",
    "step_size_in_sec = 0.1\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith('.wav'):\n",
    "        audio_file_path = os.path.join(input_path, filename)\n",
    "        csv_label_path = os.path.join(input_path, filename.replace('.wav', '_labels.csv'))\n",
    "        num_segments, num_zero_labels, num_one_labels = process_audio_file(audio_file_path, csv_label_path, \n",
    "                                        output_path,step_size_in_sec=step_size_in_sec, label_threshold=label_threshold)\n",
    "        tot_segments += num_segments\n",
    "        tot_zeros += num_zero_labels\n",
    "        tot_ones += num_one_labels\n",
    "        tot_files += 1\n",
    "print(\"Total number of segments:\", tot_segments)\n",
    "print(\"Total number of zero labels:\", tot_zeros)\n",
    "print(\"Total number of one labels:\", tot_ones)\n",
    "print(\"Total number of files:\", tot_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8TL-Vzuaz4cF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with label_threshold=0.2, step_size_in_sec=0.05\n",
      "Total number of segments for label_threshold=0.2, step_size_in_sec=0.05: 19062\n",
      "Total number of zero labels: 11600\n",
      "Total number of one labels: 6266\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.2, step_size_in_sec=0.1\n",
      "Total number of segments for label_threshold=0.2, step_size_in_sec=0.1: 9503\n",
      "Total number of zero labels: 5524\n",
      "Total number of one labels: 2935\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.2, step_size_in_sec=0.2\n",
      "Total number of segments for label_threshold=0.2, step_size_in_sec=0.2: 4724\n",
      "Total number of zero labels: 2630\n",
      "Total number of one labels: 1396\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.4, step_size_in_sec=0.05\n",
      "Total number of segments for label_threshold=0.4, step_size_in_sec=0.05: 19062\n",
      "Total number of zero labels: 14021\n",
      "Total number of one labels: 3845\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.4, step_size_in_sec=0.1\n",
      "Total number of segments for label_threshold=0.4, step_size_in_sec=0.1: 9503\n",
      "Total number of zero labels: 6665\n",
      "Total number of one labels: 1794\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.4, step_size_in_sec=0.2\n",
      "Total number of segments for label_threshold=0.4, step_size_in_sec=0.2: 4724\n",
      "Total number of zero labels: 3177\n",
      "Total number of one labels: 849\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.6, step_size_in_sec=0.05\n",
      "Total number of segments for label_threshold=0.6, step_size_in_sec=0.05: 19062\n",
      "Total number of zero labels: 16335\n",
      "Total number of one labels: 1531\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.6, step_size_in_sec=0.1\n",
      "Total number of segments for label_threshold=0.6, step_size_in_sec=0.1: 9503\n",
      "Total number of zero labels: 7740\n",
      "Total number of one labels: 719\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.6, step_size_in_sec=0.2\n",
      "Total number of segments for label_threshold=0.6, step_size_in_sec=0.2: 4724\n",
      "Total number of zero labels: 3691\n",
      "Total number of one labels: 335\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.65, step_size_in_sec=0.05\n",
      "Total number of segments for label_threshold=0.65, step_size_in_sec=0.05: 19062\n",
      "Total number of zero labels: 16779\n",
      "Total number of one labels: 1087\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.65, step_size_in_sec=0.1\n",
      "Total number of segments for label_threshold=0.65, step_size_in_sec=0.1: 9503\n",
      "Total number of zero labels: 7946\n",
      "Total number of one labels: 513\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.65, step_size_in_sec=0.2\n",
      "Total number of segments for label_threshold=0.65, step_size_in_sec=0.2: 4724\n",
      "Total number of zero labels: 3782\n",
      "Total number of one labels: 244\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.7, step_size_in_sec=0.05\n",
      "Total number of segments for label_threshold=0.7, step_size_in_sec=0.05: 19062\n",
      "Total number of zero labels: 17122\n",
      "Total number of one labels: 744\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.7, step_size_in_sec=0.1\n",
      "Total number of segments for label_threshold=0.7, step_size_in_sec=0.1: 9503\n",
      "Total number of zero labels: 8110\n",
      "Total number of one labels: 349\n",
      "Total number of files processed: 110\n",
      "Processing with label_threshold=0.7, step_size_in_sec=0.2\n",
      "Total number of segments for label_threshold=0.7, step_size_in_sec=0.2: 4724\n",
      "Total number of zero labels: 3865\n",
      "Total number of one labels: 161\n",
      "Total number of files processed: 110\n"
     ]
    }
   ],
   "source": [
    "# create tons of files\n",
    "label_thresholds = [0.2, 0.4, 0.6, 0.65, 0.7]\n",
    "step_sizes_in_sec = [0.05, 0.1, 0.2]\n",
    "\n",
    "for label_threshold in label_thresholds:\n",
    "    for step_size_in_sec in step_sizes_in_sec:\n",
    "        print(f\"Processing with label_threshold={label_threshold}, step_size_in_sec={step_size_in_sec}\")\n",
    "        tot_segments = 0\n",
    "        tot_zeros = 0\n",
    "        tot_ones = 0\n",
    "        tot_files = 0\n",
    "        \n",
    "        for filename in os.listdir(input_path):\n",
    "            if filename.endswith('.wav'):\n",
    "                audio_file_path = os.path.join(input_path, filename)\n",
    "                csv_label_path = os.path.join(input_path, filename.replace('.wav', '_labels.csv'))\n",
    "                num_segments, num_zero_labels, num_one_labels = process_audio_file(\n",
    "                    audio_file_path, csv_label_path, output_path,\n",
    "                    step_size_in_sec=step_size_in_sec, label_threshold=label_threshold\n",
    "                )\n",
    "                tot_segments += num_segments\n",
    "                tot_zeros += num_zero_labels\n",
    "                tot_ones += num_one_labels\n",
    "                tot_files += 1\n",
    "        \n",
    "        print(f\"Total number of segments for label_threshold={label_threshold}, step_size_in_sec={step_size_in_sec}: {tot_segments}\")\n",
    "        print(f\"Total number of zero labels: {tot_zeros}\")\n",
    "        print(f\"Total number of one labels: {tot_ones}\")\n",
    "        print(f\"Total number of files processed: {tot_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tensor files correctly match their corresponding label files.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tensor shape: torch.Size([3, 128, 128])\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomTensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Re-create the datasets and data loaders\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTensorDataset\u001b[49m(tensor_paths, label_paths)\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Inspect one batch of data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomTensorDataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
